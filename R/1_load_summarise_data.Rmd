---
title: "1_load_summarise_data"
output:
  html_document:
    code_folding: hide
author: mlagisz
date: "16/09/2022"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}

sessionInfo()

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
#devtools::install_github("massimoaria/bibliometrix") # to install the bibliometrix most recent version from GITHUB
#install.packages('rAltmetric') #devtools::install_github("ropensci/rAltmetric") 

pacman::p_load(tidyverse, 
               here,
               rgbif,
               visdat, #for vis_miss
               roadoi, #https://cran.r-project.org/web/packages/roadoi/vignettes/intro.html
               corpcor,
               naniar,
               rvest,
               httr,
               bibliometrix,
               networkD3,
               stringr,
               tibble,
               tidystringdist,
               synthesisr,
               ggplot2,
               RColorBrewer,
               rAltmetric,
               maps,
               purrr,
               ggthemr)

ggthemr('pale') #select one ggplot theme to be used

#for selecting colourblind-friendly pallettes:
#library("scales")
#library("ggplot2")
#library("ggthemes")
show_col(colorblind_pal()(7)) #change number of required colors
#+ scale_colour_colorblind()
```

# ALA impact project  
 
This code is for data cleaning and pre-processing from data from custom-curated Zotero library and other sources.   

Aim: Characterize  the impacts and benefits of local biodiversity information service (Living Atlas/ALA).   


These are  main data sources to be used in this project:   
1. ALA impact literature collection, hosted on Zotero as a shared group library - data/ALA_4424_Exported Items_20220722.csv and data/ALA_4424_Exported Items_20220722.ris.      
2. [ALA](https://www.ala.org.au/) data via [API services](https://www.ala.org.au/govhack/). Note that there is a new R package [galah](https://atlasoflivingaustralia.github.io/ALA4R/) for accessing biodiversity-related records. The old package [ALA4R](https://atlasoflivingaustralia.github.io/ALA4R/) is now depreciated.   


## ALA - initial overview  

As a starting point use the infographics "ALA (2020) 10 years in stats. Celebrating a decade of delivering trusted biodiversity data to support research and decision-making" (20322-ALA-10-Stats-for-10-Years-Infographic-V6.pdf). This contains ALA resources and impact summary from November 2020 (presented as a Table below). Consider presenting overlap with GBIF, where possible, and show resources and impact that is unique to ALA.

```{r table ALA overview 2020}

t1_numbers_2020 <- c("91,137,295",
                  "1,880,920",
                  "27 countries",
                  "700+",
                  "78,300",
                  "919,919",
                  "371,890",
                  "2,328",
                  "1,184,255","99% of data")

t1_labels <- c("species occurrence records", 
            "dataset downloads",
            "around the world using ALA infrastructure and source code",
            "datasets from data partners across research, government and citizen science",
            "registered users",
            "museum specimen labels transcribed by 8,529 DigiVol volunteers",
            "pages of Australian literature digitised for the Biodiversity Heritage Library",
            "scientific journal publications referencing ALA",
            "records for Australian Magpie (Gymnorhina tibicen), the most recorded species in the ALA",
            "in the ALA have Creative Commons licences")

t1 <-  tibble(t1_labels, t1_numbers_2020) %>% DT::datatable(colnames = c("label","data"), rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '200px', pageLength = 6), caption = "Table 1. Data from ALA 2020 infographics - to be updated") 
t1

```

**TODO HERE:**    
- update table with 2022 data for ALA. Try [Dashboard data feed](https://dashboard.ala.org.au/dashboard/data)   
- collate equivalent data for GBIF?       

----

## ALA - load dataset for impact assessment   

Load  list of "impact" literature related to ALA - use the .csv file exported from Zotero group library (22/07/2022 "ALA - Cited" Library).   

**NOTE:** Cannot use the files exported from Zotero with "convert2df" function from the bibliometrix package. Try later to convert the data frame from this .csv file file into the internal *bibliometrix* format.   

```{r upload original zotero references from csv, eval=TRUE}
dat <- read.csv(here("data", "ALA_4424_Exported Items_20220722.csv"))
dim(dat) #4423 87
#names(dat)

#add a column in the main data frame article vs. grey
dat %>% mutate(article_grey = case_when(Item.Type == "journalArticle" ~ "article", 
                                Item.Type != "journalArticle" ~ "grey")) -> dat
table(dat$article_grey)

table(dat$article_grey, dat$Abstract.Note=="") # only 670 of grey have abstract, 2619 articles have abstract
```

Create simple tables summarizing key selected variables.   

```{r summarise overall, results = 'hide'}
table(dat$Item.Type, useNA = "always")
#hist(dat$Publication.Year) #make graph by year, below
table(dat$Language, useNA = "always") #needs cleaning, below
table(dat$Place, useNA = "always") #needs cleaning, see below, but 3876 are empty
table(dat$Manual.Tags, useNA = "always") #split into individual later
table(str_starts(dat$DOI, "10.")) #2835 are proper DOI numbers, other are "" or "-"
#table(is.na(dat$References)) #none
#dat %>% filter(Item.Type == "conferencePaper") %>% View() #check out conference Ppapers
```

```{r clean doi info, results = 'hide'}
names(dat)
dat$DOI[dat$DOI == "-"] <- "" #replace with empty value for consistency
dat$DOI[dat$DOI == "n/a"] <- "" #replace with empty value for consistency
dat$DOI <- gsub("https://doi.org/", "", dat$DOI, fixed = TRUE)  #replace with ""
dat$DOI <- gsub("http://dx.doi.org/", "", dat$DOI, fixed = TRUE)  #replace with ""
length(unique(dat$DOI)) #2836

table(str_starts(dat$DOI, "10."), dat$Item.Type) #DOI by Item.Type
```

```{r clean Language, results = 'hide'}
table(dat$Language, useNA = "always") #needs cleaning, below
dat$Language <- replace(dat$Language, dat$Language %in% c("en","EN", "en_AU", "en_US", "en-AU", "en-GB", "en-IN", "en-us", "en-US", "eng", "dcterms.RFC4646; en-AU"), "English")
dat$Language <- replace(dat$Language, dat$Language %in% c("de"), "German")
dat$Language <- replace(dat$Language, dat$Language %in% c("fr"), "French")
dat$Language <- replace(dat$Language, dat$Language %in% c("es"), "Spanish")
dat$Language <- replace(dat$Language, dat$Language %in% c("por"), "Portugese")
dat$Language <- replace(dat$Language, dat$Language %in% c("id"), "Indonesian")
dat$Language <- replace(dat$Language, dat$Language %in% c("IT"), "Italian")

#table(dat$Language, useNA = "always") #much better, but 2003 records have no language info
table(dat$Language)["English"] #2401 records with English
#table(dat$Language)[""]
sum(dat$Language=="") #2003 records without language info
dat$Language <- replace(dat$Language, dat$Language %in% c(""), "not specified")
table(dat$Language, useNA = "always") #much better, but 2003 records as "not specified"
```

Simple bar plot for languages
  
```{r plot language simple, fig.width = 10, fig.height = 10, fig.cap = "Figure 2.", results = 'hide', include = FALSE}
### Summarize literature type information  
#par(mar = c(2, 10, 2, 2))
#barplot(table(dat$Language), horiz = TRUE, las=1,  main = "Number of papers per species") #how many articles per species
dat %>% 
  count(Language) -> count_Language
ggplot(count_Language, aes(x = reorder(Language, n), y = n)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") 
```

Language: `r table(dat$Language)["English"]` out of `r sum(table(dat$Language))` are in English, but for `r  sum(dat$Language=="not specified")` records the language info is missing.  

Place /(probably study place): some recorded at different, but mostly missing.

```{r clean Place, eval = FALSE}
table(dat$Place, useNA = "always") #needs cleaning, below
table(dat$Place=="") #only 547 records have recorded info on the place - not very useful column
all_countries <- str_c(unique(iso3166$sovereignty), collapse = "|") #use iso3166 data set
df <- dat
#all_countries <- str_c(unique(world.cities$country.etc), collapse = "|")
df$country <- sapply(str_extract_all(df$Place, all_countries), toString)
table(df$country, useNA = "always") #needs much more cleaning - try again 
```

Simple bar plot for literature type
  
```{r plot lit type simple, fig.width = 10, fig.height = 10, fig.cap = "Figure 2.", results = 'hide', include = FALSE}
### Summarize literature type information  
#par(mar = c(2, 10, 2, 2))
#barplot(table(dat$Item.Type), horiz = TRUE, las=1,  main = "Number of papers per species") #how many articles per species
dat %>% 
  count(Item.Type) -> count_lit_type
ggplot(count_lit_type, aes(x = reorder(Item.Type, n), y = n)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") 
```

### Fill in missing year info

```{r fill in year, results = 'hide'}
#View(dat[is.na(dat$Publication.Year), c("Key", "Item.Type", "DOI", "Url", "Title")])
#all missing values should be 2022
#table(is.na(dat$Publication.Year))
dat$Publication.Year[is.na(dat$Publication.Year)] <- 2022
```

### Clean string characters
Clean some common foreign characters in the dataset.

```{r clean foreign characters, eval=TRUE}
#only use latin-ascii characters
dat$Author <- stringi::stri_trans_general(dat$Author, 'latin-ascii')
dat$Title <- stringi::stri_trans_general(dat$Title, 'latin-ascii')
dat$Abstract.Note <- stringi::stri_trans_general(dat$Abstract.Note, 'latin-ascii')

#remove some html formatting
dat$Title <- gsub("<i>", "", dat$Title)
dat$Title <- gsub("</i>", "", dat$Title)
```

### Clean manual tags
Manual.Tags field contains various manually assigned tags to identify how ALA was used and what was done

```{r process manual tags, results = 'hide'}
mtags <- str_split(dat$Manual.Tags, "; ") 
table(unlist(mtags))

#keep only the 6 tags starting with numerics (main use)
mtags_num <- unlist(mtags)[str_detect(unlist(mtags), "^[0-9]")]
count_mtags_num <- as.data.frame(table(mtags_num))
#str(count_mtags_num)

#plot
ggplot(count_mtags_num, aes(x = reorder(mtags_num, Freq), y = Freq)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("count") 

#Non-numeric manual tags assigned only to records where ALA was used ("1 - ALA used") - they provide details of use
mtags_det <- unlist(mtags)[unlist(mtags) %in% c("Species occurrence records", "Map" , "Modelling", "Climate Data", "Spatial Portal", "Species lists", "Profiles")]
count_mtags_det <- as.data.frame(table(mtags_det))

#plot
ggplot(count_mtags_det, aes(x = reorder(mtags_det, Freq), y = Freq)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("count") 
```


### Australan focus

Create columns indicating  if Australia-related terms (e.g. Tasmania, Victoria, Queensland, NSW, Sydney) was mentioned in study title, abstract, or both

```{r Australan focus, results = 'hide'}
#create a new logical column with TRUE when AU-relevant terms mentioned in titles 
dat$AU_title <- str_detect(string = str_to_lower(dat$Title), pattern = "austral|tasman|victoria|queensland|northern territiory|new south wales|nsw|vic|qld|sunshine coast|canberra|mudgee|pilbara|illawarra|sydney|melbourne|perth|adelaide|brisbane|great barrier reef|snowy mountains|murray-darling|gippsland|aboriginal|kangaroo|koala|platypus|echidna|quoll|eucalypt")
#dat$Title[dat$AU_title == FALSE] lists that that did not match
# table(dat$AU_title)

#create a new logical column with TRUE when AU-relevant terms mentioned in abstracts 
dat$AU_abs <- str_detect(string = str_to_lower(dat$Abstract.Note), pattern = "austral|tasman|victoria|queensland|northern territiory|new south wales|nsw|vic|qld|sunshine coast|canberra|mudgee|pilbara|illawarra|sydney|melbourne|perth|adelaide|brisbane|great barrier reef|snowy mountains|murray-darling|gippsland|aboriginal|kangaroo|koala|platypus|echidna|quoll|eucalypt")
# table(dat$AU_abs)

#create a new logical column with TRUE when AU-relevant terms mentioned in title or abstract 
dat$AU_title_abs <- ifelse(dat$AU_title == "TRUE" | dat$AU_abs == "TRUE", "TRUE", "FALSE")
# table(dat$AU_title_abs)
```

Plot where Australian context (AU) is mentioned

```{r Australan focus overall by field, results = 'hide', eval = FALSE}
#summarise and merge for plotting for 3 new columns across all records
count_AU <- as.data.frame(rbind("AU in title" = table(dat$AU_title), "AU in abstract" = table(dat$AU_abs), "AU in title or abstract" = table(dat$AU_title_abs)))
colnames(count_AU) <- c("not metioned", "mentioned")
count_AU$Australia <- rownames(count_AU)

#plot
ggplot(count_AU, aes(x = reorder(Australia, mentioned), y = mentioned)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("count") 
```


### Citizen science focus - by lit.type

Create columns indicating  if citizen science (CS) or related terms (e.g. volunteer, crowdsourcing) wer mentioned in study title, abstract, or both (cannot use "community" becouse it often means "ecological community" (of various species) not "human community" (general public or local residents, etc.)).

```{r citizen science focus, results = 'hide'}
#create a new logical column with TRUE when AU-relevant terms mentioned in title s
dat$CS_title <- str_detect(string = str_to_lower(dat$Title), pattern = "citizen|volunteer|crowdsourc")
#dat$Title[dat$CS_title == FALSE] lists that that did not match
# table(dat$CS_title)

#create a new logical column with TRUE when CS-relevant terms mentioned in abstracts 
dat$CS_abs <- str_detect(string = str_to_lower(dat$Abstract.Note), pattern = "citizen|volunteer|crowdsourc")
# table(dat$CS_abs)

#create a new logical column with TRUE when CS-relevant terms mentioned in titles or abstracts 
dat$CS_title_abs <- ifelse(dat$CS_title == "TRUE" | dat$CS_abs == "TRUE", "TRUE", "FALSE")
# table(dat$CS_title_abs)
```

Plot where citizen science (CS) is mentioned

```{r citizen science focus overall by data field, results = 'hide', eval = FALSE}
#summarise and merge for plotting for 3 new columns across all records
count_CS <- as.data.frame(rbind("CS in title" = table(dat$CS_title), "CS in abstract" = table(dat$CS_abs), "CS in title or abstract" = table(dat$CS_title_abs)))
colnames(count_CS) <- c("not metioned", "mentioned")
count_CS$CS <- rownames(count_CS)

#plot
ggplot(count_CS, aes(x = reorder(CS, mentioned), y = mentioned)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("count")  
```




### SUBSETS by literature (item) type

# Separate analyses for peer-reviewed vs. grey literature

## Subset data set

```{r subset dataset, results = 'hide'}
dat_articles <- dat %>% filter(Item.Type == "journalArticle") %>% dplyr::select(Key:Manual.Tags)
dim(dat_articles) #3093
dat_grey <- dat %>% filter(Item.Type != "journalArticle") %>% dplyr::select(Key:Manual.Tags)
dim(dat_grey) #1330
```

## Summarise data subsets 

```{r summarise articles, results = 'hide'}
length(unique(dat_articles$DOI)) #2740 DOI (but see below)
table(dat_articles$DOI, useNA = "always")[1:3] #354 with empty DOI
#View(dat_articles[dat_articles$DOI == "-", ])
length(unlist(str_split(dat_articles$Author, "; ")))
length(unique((unlist(str_split(dat_articles$Author, "; ")))))
#remove remaining completely empty columns
dat_articles %>% select_if(function(x){!all(is.na(x))}) -> dat_articles
```

```{r summarise grey, results = 'hide'}
length(unique(dat_grey$DOI)) #only 97 (but see below)
table(dat_grey$DOI, useNA = "always")[1:2] #1234 with empty DOI
#View(dat_grey[dat_grey$DOI == "-", ])
length(unlist(str_split(dat_grey$Author, "; ")))
length(unique((unlist(str_split(dat_grey$Author, "; ")))))
#remove remaining completely empty columns
dat_grey %>% select_if(function(x){!all(is.na(x))}) -> dat_grey
```


Save main processed data
```{r save processed data, results = 'hide', eval = TRUE}
readr::write_csv(dat, file = here("data","dat_all_processed.csv"))
```



The code above is now saved as "1_load_summarise_Zotero_data.Rmd"




###################################################################################
############################################ AUTHORS - START HERE

### Clean author names

Abbreviate author first names - FIX!
```{r abbreviate author first names, eval=TRUE}
#split column with author names to find most common names
authors_all <- sort(unlist(str_split(dat$Author, "; "))) #mix of full and initialised first names
#remove first word from each string (family name)
authors_all <- gsub(".*, ","", authors_all)
#extract next first word from each string (first name or initial)
authors_all <- stringr::word(authors_all, 1)
#check if string contains "." (use escape symbols!) and remove these strings
authors_all <- authors_all[!stringr::str_detect(authors_all, "\\.")]
count_authors_all <- as.data.frame(table(authors_all))
#str(count_authors_all)

#make a table of abbreviations of authors first names
authors_all_abbr <- abbreviate(authors_all, 1, named = TRUE, strict = TRUE, dot = TRUE)
#names(authors_all_abbr)
#str(authors_all_abbr)
authors_all_abbr <- authors_all_abbr[nzchar(authors_all_abbr)] #remove empty values

#abbreviate authors first names in the main data frame (as new column) using the table above 
test <- str_replace_all(dat$Author[1:10], names(authors_all_abbr), authors_all_abbr) #FIX! (add space and )
dat$Author_abbr <- gsub("\\. ", "\\.", dat$Author_abbr) #remove white space between initials
```

### Author collaboration network

```{r  author collaboration network, eval=FALSE}
Author_split <- str_split(dat$Author_abbr, "; ")
df_auth_collab <- data.frame(ref.id = rep(dat$Key, lengths(Author_split)),
                  auth_names = unlist(Author_split))
str(df_auth_collab)
#df_auth_collab[1:20, ]

#remove empty ones 
df_auth_collab <- df_auth_collab[!is.na(df_auth_collab$auth_names), ]
df_auth_collab <- df_auth_collab[df_auth_collab$auth_names != "", ]

library(igraph)
df_auth_collab %>%
  inner_join(df_auth_collab, by = "ref.id") %>%
  filter(auth_names.x < auth_names.y) %>%
  count(auth_names.x, auth_names.y) %>%
  graph_from_data_frame(directed = FALSE) -> g1 #with author names as vertices
summary(g1)
#overall graph statistics
graph.density(g1) #0.0015 = quite sparse
transitivity(g1, type = "global") # 0.936 = probability that the neighbors of a node are also connected (also called the clustering coefficient)

#visualize our co-authorship graph network
#g1_graph_lkk <- layout.kamada.kawai(g1) 
g1_graph_nicely <- layout_nicely(g1) #choose an appropriate graph layout algorithm for the graph automatically
plot(g1, layout = g1_graph_nicely, vertex.label = NA, vertex.size = 2, vertex.color = "grey")
```



#mtags_det <- unlist(mtags)[unlist(mtags) %in% c("Species occurrence records", "Map" , "Modelling", "Climate Data", "Spatial Portal", "Species lists", "Profiles")]
#count_mtags_det <- as.data.frame(table(mtags_det))







############ ARTICLES-FOCUSED SECTION

### Articles - get extra data via DOI from Unpaywall

Articles - Use DOI to integrate with additional data available from Unpaywall via [roadoi R package](https://docs.ropensci.org/roadoi/reference/roadoi-package.html).   

```{r article doi data, results = 'hide', eval = FALSE}
#table(is.na(dat_articles$DOI)) #no missing DOI values

## fetch data [SKIP]
#dois <- unique(dat_articles$DOI) #some values need cleaning, as not proper DOI (empty strings)
#refs_data <- roadoi::oadoi_fetch(dois = dois, email = "losialagisz@gmail.com") #get refs data from doi

#names(refs_data)
#str(refs_data) #list of data frames with lists of data frames
#get failed DOIs
#str(warnings()) #List of 35 warnings - mainly that an article is not in Unpaywall
#failed_DOIs <- warnings()

## load from a stored file instead:
#readr::write_csv(refs_data, file = here("data","refs_data.csv"))
refs_data <- readr::read_csv(file = here("data","refs_data.csv"))
names(refs_data)
refs_data$is_oa
refs_data$oa_status
refs_data$has_repository_copy
refs_data$journal_is_oa
refs_data$journal_name
#refs_data$doi
#dat_articles$DOI

#check overlap by DOI
length(intersect(refs_data$doi, dat_articles$DOI)) ##2240 out of 3093 are matching
length(setdiff(refs_data$doi, dat_articles$DOI)) #12
length(setdiff(dat_articles$DOI, refs_data$doi)) #500

#check overlap by title
length(intersect(refs_data$title, dat_articles$Title)) ##1377 out of 3093 are matching
length(setdiff(refs_data$title, dat_articles$Title)) #611
length(setdiff(dat_articles$Title, refs_data$title)) #1712
#overlap by DOI is much better

refs_data$DOI <- refs_data$doi

#match and merge data to dat_articles by DOI
dat_articles_refs <- left_join(dat_articles, refs_data, by = "DOI")
dim(dat_articles_refs) #3093 58
#str(dat_articles_refs)
#names(dat_articles_refs)
#readr::write_csv(dat_articles_refs, file = here("data","dat_articles_refs.csv"))
#dat_articles_refs <- readr::read_csv(file = here("data","dat_articles_refs.csv"))

# #check title overlap
# table(dat_articles$Title == dat_articles$title, useNA = "always")
# View(dat_articles_refs[dat_articles$Title != dat_articles$title, c("Title", "title")]) #similar - diff. in special characters ets.

# #check year overlap
#  table(dat_articles$Publication.Year == dat_articles$year, useNA = "always")
#  View(dat_articles_refs[dat_articles$Publication.Year != dat_articles$year, c("Publication.Year", "year")]) #similar - diff. usually is  1 year

# #check journal name overlap
#  table(dat_articles$Publication.Title == dat_articles$journal_name, useNA = "always")
#  View(dat_articles_refs[dat_articles$Publication.Title != dat_articles$journal_name, c("Publication.Title", "journal_name")]) #similar - usually diff. capitalisation or special chars


#remove empty or uninformative columns
dat_articles_refs %>% select(-c(Date, Date.Added, Date.Modified, Access.Date, Journal.Abbreviation, Num.Pages, Short.Title, Series, Series.Title, Publisher, Place, Rights, Type, Archive, Archive.Location, Library.Catalog, Call.Number, Link.Attachments, Notes, doi, is_paratext, updated_resource, oa_locations, best_oa_location, oa_locations_embargoed, data_standard, authors, journal_issns, journal_issn_l, genre, title, year, published_date, journal_name)) -> dat_articles
```

### Plot article and journals Open Access status info 

Meta-data available on https://unpaywall.org/data-format
```{r OA plots, results = 'hide'}
names(dat_articles)

#one-way tables
table(dat_articles$is_oa, useNA = "always")
table(dat_articles$oa_status, useNA = "always")
table(dat_articles$has_repository_copy, useNA = "always") #Is a full-text available in a repository? TRUE if this resource has at least one OA Location with host_type = "repository"
table(dat_articles$journal_is_oa, useNA = "always")

#two-way tables
table(dat_articles$is_oa, dat_articles$oa_status, useNA = "always")
#table(dat_articles$is_oa, dat_articles$has_repository_copy, useNA = "always")
table(dat_articles$is_oa, dat_articles$journal_is_oa, useNA = "always") #looks like the one above
#table(dat_articles$oa_status, dat_articles$journal_is_oa, useNA = "always")
table(dat_articles$oa_status, dat_articles$has_repository_copy, useNA = "always")


#count values by 2 columns
count_dat_articles_OA <- count(dat_articles, is_oa, oa_status, name = "Count", sort = TRUE) #%>% mutate(oa_status = str_replace(oa_status, "NA", "no information"))
#count_dat_articles_OA$is_oa <- factor(as.character(count_dat_articles_OA$is_oa))
str(count_dat_articles_OA)
count_dat_articles_OA$is_oa[is.na(count_dat_articles_OA$is_oa)] <- "no information"
count_dat_articles_OA$oa_status[is.na(count_dat_articles_OA$oa_status)] <- "no information"
count_dat_articles_OA$is_oa <- as.factor(count_dat_articles_OA$is_oa)
levels(count_dat_articles_OA$is_oa) <- c("closed", "no information", "open access")
count_dat_articles_OA$oa_status <- factor(count_dat_articles_OA$oa_status, levels=c("no information", "closed", "hybrid", "gold", "bronze", "green"))
#count_dat_articles_OA <- count_dat_articles_OA[count_dat_articles_OA$is_oa != "NA",]

# #plot as proportion
# ggplot(count_dat_articles_OA, aes(x = reorder(is_oa, Count), y = Count, fill = oa_status)) + 
#   geom_bar(position = "fill", stat = "identity") +
#   coord_flip() +
#   xlab("")  +
#   ylab("proportion") 
# 
# #plot as counts
# ggplot(count_dat_articles_OA, aes(x = reorder(is_oa, -Count), y = Count, fill = oa_status)) + 
#   geom_bar(stat = "identity") +
#   coord_flip() +
#   xlab("")  +
#   ylab("article count") 

# Plot as counts with Custom Color Palette (color-blind friendly) and labels
Colors_OA <-  c("no information" = "#999999", "closed" = "#000000", "gold" = "#E69F00", "green" = "#009E73", "bronze" = "#D55E00", "hybrid" = "#56B4E9")
ggplot(count_dat_articles_OA, aes(x = reorder(is_oa, Count), y = Count, fill = oa_status)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  scale_fill_manual(values = Colors_OA, name = 'open access type:', breaks = c('green', 'bronze', 'gold',  'hybrid'), labels = c("Green = unpublished version freely available via a repository", "Bronze = published version is free to read only on the publisher's website", "Gold = author pays for a free-to-read version to be available from the fully-OA journal", "Hybrid = author pays for a free-to-read version to be available from the subscription journal")) +
  geom_text(aes(label = Count), color = "white", size = 3, position = position_stack(vjust = 0.5)) + 
  theme(legend.position = "bottom", legend.direction = "vertical") +
  ylab("article count") 

ggsave(file = here("plots", "plot.article_OA.pdf"), width = 12, height = 7, units = "cm", dpi = "retina", scale = 1.6)
```


### Article - clean journal names (preliminary)

```{r clean journal names, eval=TRUE}
#dat_articles %>% count(Publication.Title, name = "Count") %>%  View()
dat_articles$Journal <- dat_articles$Publication.Title #create a new column to store cleaned journal names

#start fixing some journal names
dat_articles$Journal <- gsub("botanica", "Botanica", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("PLANTS", "Plants", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(" & ", " and ", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(" : the Journal of Integrative Biogeography", "", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("BioRxiv", "bioRxiv", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("Elife", "eLife", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("Emu - Austral Ornithology", "Emu", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(" (Gesellschaft für Informatik)", "", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("systematics", "Systematics", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("John Wiley & Sons", "", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(": Cultural Heritage Series", "", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("Mycokeys", "MycoKeys", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("Neobiota", "NeoBiota", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("coastal management", "Coastal Management", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("the", "The", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub("PLOS", "PLoS", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(", The", "", dat_articles$Journal, fixed = TRUE)
dat_articles$Journal <- gsub(": X", "", dat_articles$Journal, fixed = TRUE)
```

```{r plot top journal names, eval=TRUE}
#get top 20 journals
dat_articles %>% count(Journal, name = "Count") %>% arrange(desc(Count)) %>% slice_head(n = 20) -> dat_articles_top20 #%>% View
dat_articles_top20

#plot as counts
ggplot(dat_articles_top20, aes(x = reorder(Journal, Count), y = Count)) + #, fill = "grey"
  geom_bar(stat = "identity") +
  theme(legend.position = "bottom") +
  coord_flip() +
  xlab("")  +
  ylab("article count") 

#TODO - consider adding more info - e.g. Australian? IF?
```


### Articles - Journal IF
Data based on JCR 2022 Impact Factors: https://impactfactorforjournal.com/jcr-impact-factor-2022/ (this link will change every year - and only top 100 are shown).
Use data for 2022 also saved as pdf: JCR-2021-Impact-Factor-PDF.pdf (password protected for exports) with IF for 10,593 journals.
Note: tabulizer is not longer supported and requires JVR for R, so use pdftools package  instead.
```{r get journals IF from pdf table}
#install.packages("pdftools")
library(pdftools)

# Extract the table
pdf_IF_table <- pdf_text(here("data", "JCR-2021-Impact-Factor-PDF.pdf"))
#str(pdf_IF_table) #a character vector
length(pdf_IF_table) #222
head(pdf_IF_table)

pdf_IF_table2 <- strsplit(pdf_IF_table, "\n") #split on evey newline mark
#str(pdf_IF_table2) #a list 
#head(pdf_IF_table2)
pdf_IF_table2 <- unlist(pdf_IF_table2) #change list into a single vector of stinggs
pdf_IF_table2[1:10]
pdf_IF_table2 <- pdf_IF_table2[-c(1:6)] #remove first 6 elements with table headers and description
pdf_IF_table2 <- trimws(pdf_IF_table2,  which = "left") #remove unnecessary leading white spaces
pdf_IF_table2 <- str_split(pdf_IF_table2, " {2,}") #split at oints with at least two white spaces
table(lengths(pdf_IF_table2)) #there are few hundreds of misaligned records (1 or 2 strings only) - need to be removed
pdf_IF_table2 <- pdf_IF_table2[as.logical(lengths(pdf_IF_table2) == 3)] #keep only the list elements with 3 sub-elements
pdf_IF_df <- do.call(rbind.data.frame, pdf_IF_table2) #convert list to data frame
#str(pdf_IF_df)
names(pdf_IF_df) <- c(
  "nr",
  "journal_name",
  "journal_IF"
)

pdf_IF_df[grepl("[0-9].[0-9]", pdf_IF_df$journal_name), ] #journal titles with at least 2 numbers
pdf_IF_df <- pdf_IF_df[!grepl("[0-9].[0-9]", pdf_IF_df$journal_name), ] #remover rows where journal titles have at least 2 numbers

names(pdf_IF_df)
pdf_IF_df$journal_name <- str_to_upper(pdf_IF_df$journal_name)
pdf_IF_df$journal_IF <- as.numeric(pdf_IF_df$journal_IF)
#hist(pdf_IF_df$journal_IF)
#pdf_IF_df[pdf_IF_df$journal_IF > 100, ]

#check overlap of journal names
dat_articles$JOURNAL <- str_to_upper(dat_articles$JOURNAL) #create new column with journal names in uppercase
length(unique(dat_articles$JOURNAL)) #869
length(intersect(pdf_IF_df$journal_name, dat_articles$JOURNAL)) ##474 out of 869 are matching
length(setdiff(dat_articles$JOURNAL, pdf_IF_df$journal_name)) #395 out of 869 not matching

#check for & symbol in journal names
#grep("&", pdf_IF_df$journal_name, value = TRUE) #lots!
pdf_IF_df$JOURNAL <- pdf_IF_df$journal_name #new column to store modified journal names 
pdf_IF_df$JOURNAL <- gsub(" & ", " and ", pdf_IF_df$JOURNAL, fixed = TRUE)
#check overlap of journal names
length(intersect(pdf_IF_df$JOURNAL, dat_articles$JOURNAL)) ##474 out of 869 are matching
length(setdiff(dat_articles$JOURNAL, pdf_IF_df$JOURNAL)) #395 out of 869 not matching

#match and merge data to dat_articles by uppercase journal name
dat_articles <- left_join(dat_articles, pdf_IF_df, by = "JOURNAL")
dim(dat_articles) #3093 58
#str(dat_articles)
#names(dat_articles)
#readr::write_csv(dat_articles, file = here("data","dat_articles.csv"))
#dat_articles <- readr::read_csv(file = here("data","dat_articles.csv"))

# check IF data missingness 
table(is.na(dat_articles$journal_IF)) #1046 articles have IF, 2047 no

#remove empty or uninformative columns
dat_articles %>% select(-c(nr, journal_name)) -> dat_articles
```

Summarise IF data

```{r summarise journals IF}
#only keep records with IF data
dat_articles %>% drop_na(journal_IF) -> dat_articles_IF
dim(dat_articles_IF)[1] #2047 records
n_distinct(dat_articles_IF$Journal) #474 different journals
max(dat_articles_IF$journal_IF) #69.504 highest IF

names(dat_articles_IF)

#get records for journals with IF>10
dat_articles_IF %>% arrange(desc(journal_IF)) %>% select(Journal, journal_IF, Manual.Tags, Title) %>% filter(journal_IF > 10) -> dat_articles_IFover10
dim(dat_articles_IFover10)[1] #147
dim(dat_articles_IFover10)[1]/dim(dat_articles_IF)[1]*100 # 147 is 7% of articles with IF are in journals with IF >10 (27 journals)
dat_articles_IFover10 %>% add_count(Journal, name = "Count") %>% arrange(desc(Count)) -> dat_articles_IFover10 #counts added as a column to records
dat_articles_IFover10 %>% distinct(Journal, .keep_all = TRUE) %>% arrange(desc(Count)) -> dat_articles_IFover10_journals #counts as new data frame of journals
dim(dat_articles_IFover10_journals)[1] #27 journals with >10 IF

#plot as counts
ggplot(dat_articles_IFover10_journals, aes(x = reorder(Journal, Count), y = Count, fill = journal_IF)) + #, fill = "grey"
  geom_bar(stat = "identity") +
  theme(legend.position = "top") +
  coord_flip() +
  xlab("")  +
  ylab("article count") +
  scale_fill_gradient(name = '>10 journal impact factor:', low = "lightblue", high = "darkblue")

#TODO - do this for the most popular journals and use bold font for regional ones? see https://community.rstudio.com/t/bold-or-italicize-some-axis-text-ggplot2/126143/2
```


### Articles - Journal Quantile and Rank from Scimago

Data from Scimago website https://www.scimagojr.com/journalrank.php - donloaded as "scimagojr 2021.csv"
```{r get journals Quantile and Rank}
dat_scimago <- readr::read_delim(file = here("data","scimagojr 2021.csv"), delim = ";")
#str(dat_scimago)
#names(dat_scimago)
# dat_scimago$Rank
# table(dat_scimago$"SJR Best Quartile")
# dat_scimago$"H index"
# table(dat_scimago$Country)
# table(dat_scimago$Region)
# dat_scimago$Categories[1:10]

#clean-up table: 
dat_scimago$Title <- str_replace_all(dat_scimago$Title, "[\r\n]" , " ") %>% str_to_upper() #remove newline symbols and change all to uppercase
#attributes(dat_scimago) #check attributes
attr(dat_scimago,'problems') <- NULL #remove problems attribute from the IF column
#table(is.na(dat_scimago$Title)) #0 missing  values


#names(dat_articles)
#names(dat_scimago)
# glimpse(dat_scimago) #all capitals
# dat_scimago$Title
# dat_scimago$"SJR Best Quartile"
# dat_scimago$Country
# table(dat_scimago$Region)
# dat_scimago$Categories #needs to be strsplit

#check the overlap of journal names in two data frames
dat_articles$JOURNAL %in% dat_scimago$Title -> fit
sum(fit) #2534found
sum(!fit) #559 not found
#table(dat_articles$JOURNAL[!fit]) #journals not found in Scimago (mostly local ones, some preprint repos, etc.)

### testing - find "EMU": "EMU" %in% dat_scimago$Title #TRUE
#dat_scimago[dat_scimago$Title=="EMU", ]
#table(str_detect(dat_scimago$Title, "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES")) #6 partial matches
#dat_scimago$Title[str_detect(dat_scimago$Title, "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES")] #showing matching names - "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA"  can be fixed
dat_scimago$Title <- str_replace(dat_scimago$Title, "^PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA$", "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES")
#dat_scimago[dat_scimago$Title=="PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES", ]
dat_scimago$JOURNAL <- dat_scimago$Title #new column for easier merging
dat_scimago %>% select(-c("Title", "Sourceid", "Type", "Issn", "H index", "Total Docs. (2021)", "Total Docs. (3years)", "Total Refs.", "Total Cites (3years)", "Citable Docs. (3years)", "Cites / Doc. (2years)",  "Ref. / Doc.", "Publisher", "Coverage")) -> dat_scimago

#merge data frames to append journal IF to my publications table
dat_articles <- left_join(dat_articles, dat_scimago, by = c("JOURNAL"))
#str(dat_articles)

dat_articles$SJR <- as.numeric(gsub(",", ".", dat_articles$SJR)) #change to numeric scores
```

Plot  summaries of Scimago journal data  
```{r summarise Scimago journal SJR score yearly data, eval = FALSE}
# simple histogram of frequencies of SJR  in my publication list
# dat_articles %>% ggplot(aes(SJR)) + geom_histogram(binwidth = 1, fill="grey") 
# + theme_minimal()

#library(ggrepel)
# scatterplot of jurnal SJR and names over publication years
pos <- position_jitter(width = 0.1, height = 0.1, seed = 1) #used below to add jitter (does not work withinn plot lines?)
dat_articles %>% filter(!is.na(SJR)) %>% ggplot(aes(x = Publication.Year, y = SJR)) + geom_point(alpha = 0.5, position = pos) +
  #geom_smooth(method = "lm", color = "grey") + 
  theme_minimal()  +
  ggrepel::geom_text_repel(aes(label = Journal, color = "grey"), size = 3, max.overlaps = 10) + theme(legend.position = "none")
```

```{r summarise Scimago journal SJR Best Quartile data}
names(dat_articles)

#counts per best Q category "SJR Best Quartile"
data.frame(table(dat_articles$"SJR Best Quartile")) %>% rename(Category = Var1, Count = Freq) %>% arrange(-Count) %>% filter(Category != "-") %>% 
ggplot(aes(x = reorder(Category, Count), y = Count)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("article count") 

#counts per journal Region
data.frame(table(dat_articles$Region)) %>% rename(Category = Var1, Count = Freq) %>% arrange(-Count) %>% slice_head(n = 20) %>% 
ggplot(aes(x = reorder(Category, Count), y = Count)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("article count") 

#Mosaic plot for counts per journal Q and Region
dat_articles$SJR_Best_Quartile <- dat_articles$"SJR Best Quartile"
dat_articles$SJR_Best_Quartile[dat_articles$SJR_Best_Quartile == "-"] <- NA
table(dat_articles$SJR_Best_Quartile, useNA = "always")
table(dat_articles$Region, useNA = "always")
dat_articles$Region <- as.factor(dat_articles$Region)
levels(dat_articles$Regions) <- c("Other", "Other", "Other", "Other", "Other", "Northern America", "Pacific Region", "Western Europe")

# install.packages("devtools")
#devtools::install_github("haleyjeppson/ggmosaic")
library(ggmosaic)
dat_articles %>% filter(!is.na(SJR_Best_Quartile)) %>% ggplot() +
  geom_mosaic(aes(x = product(Region, SJR_Best_Quartile), fill = Regions)) +
  theme_mosaic() + 
  theme(legend.position = "none")

#counts by main use type (some articles have more than one, needs cleaning)
dat_articles_jcat <- str_split(dat_articles$Categories, "; ") %>% unlist() #separate the categories
dat_articles_jcat <- substring(dat_articles_jcat, 1, nchar(dat_articles_jcat)-5) #remove last 4 characters
#get top 20 most common journal categories and plot
data.frame(table(dat_articles_jcat)) %>% rename(Category = dat_articles_jcat, Count = Freq) %>% arrange(-Count) %>% slice_head(n = 20) %>% 
ggplot(aes(x = reorder(Category, Count), y = Count)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")  +
  ylab("article count") 
```







## Altmetrics
Gather altmetrics data using [rAltmetric R package](https://cran.r-project.org/web/packages/rAltmetric/index.html)
Package manual: http://cran.nexr.com/web/packages/rAltmetric/rAltmetric.pdf and example: https://www.r-project.org/nosvn/pandoc/rAltmetric.html
NOTE: rAltmetric package was "archived on 2022-06-02 at the maintainer's request".    
API Altmetric info: https://api.altmetric.com/docs/call_fetch.html


Interpreting Altmetrics key output statistics:   

- score - "Altmetric score" = the main Altmetric attention score    
- cited_by_policies_count - "Number of policy sources that have mentioned the publication"  
- cited_by_msm_count - "Number of the news sources that have mentioned the publication"  
- cited_by_feeds_count - "Number of blogs that have mentioned the publication"  
- cited_by_tweeters_count - "Number of the twitter accounts that have tweeted this publication"  
- cited_by_fbwalls_count - "Number of the pages that have shared on Facebook"  
- cited_by_rdts_count - "Number of Reddit threads posted about this publication"  
- cited_by_videos_count - "Number of the Youtube/Vimeo channels"  
- cited_by_gplus_count - "Number of the accounts that have shared on Google+"   

```{r get altmetric data for orcid doi, eval = FALSE}

#dat_articles_refs <- readr::read_csv(file = here("data","dat_articles_refs.csv"))

dois2 <- as.list(dat_articles$DOI) #make a list of doi
dois2 <- dois2[dois2 != "-"] #remove empty values
length(dois2)

#get data from altmetrics server
altmetrics_lists2 <- dois2 %>% purrr::map( ~ tryCatch( {altmetrics(doi = .x)},
                                             error = function(e) {
                                             cat(paste0("•NOT_FOUND• ", .x, "\n"))
                                             }))
length(altmetrics_lists2) #3093

altmetrics_lists3 <- altmetrics_lists2 #work on a copy

#convert list of altmetrics results to cleaner dataframe
altmetrics_lists3[sapply(altmetrics_lists3, is.null)] <- NULL
altmetrics_df3 <- altmetrics_lists3 %>% map_df(~rAltmetric::altmetric_data(.x))
dim(altmetrics_df3) #49 rows

#names(altmetrics_df)[1:110] #too many author names columns after 110!
readr::write_csv(altmetrics_df3, file = here("data","dat_articles_altmetrics.csv"))
```

Summarize altmetrics data.   

```{r summarise altmetric data}

dat_articles_refs <- readr::read_csv(file = here("data","dat_articles_refs.csv"))
altmetrics_df2 <- readr::read_csv(file = here("data","papers_2020_altmetrics.csv"))

#plot a histogram of the Altmetric scores (code from https://www.r-bloggers.com/2015/09/altmetrics-on-citeulike-entries-in-r/):
hist(as.numeric(altmetrics_df2$score), main="", xlab="Altmetric score", breaks=50)

#max(as.numeric(altmetrics_df2$score)) #2654.856

#table(altmetrics_df2$cited_by_policies_count) # 7 documents cited once each

# #plot of the percentile of papers in the journal it was published in and for the full Altmetric.com corpus:
# plot(
#   as.vector(altmetrics_df2$context.all.pct),
#   as.vector(altmetrics_df2$context.journal.pct),
#   xlab="pct all", ylab="pct journal"
# )
# abline(0,1) 

#altmetrics_df2$published_on - convert from character to date format
#altmetrics_df2$authors1

t2 <-  altmetrics_df2 %>% filter(as.numeric(score) > 500) %>% select(title, journal, doi, score, starts_with("cited")) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table 2. Altmetrics for ALA impact articles published in 2020 - papers with score > 500") 
t2
```





########## Articles - Authors ### move to STEP 2!!!

```{r process author names and affiliations, results = 'hide', eval = FALSE}
str(refs_data$authors[[2]])
str(refs_data$affiliations[[1]])
refs_data$authors[[21]]

sum(is.na(dat_articles_refs$authors))
dat_articles_refs$authors[15]
```



################################ TRY LENS

## Query for LENS API using article titles

*DO NOT RUN* - the outpt was saved to a file at the end of this chunk - this output is loaded at the start of the next chunk, to save long waiting time during LENS API query.   

```{r query LENS for full records, eval = FALSE}
#subset of first 50  publications, (used for testing)
#dat50 <- dat[1:50, ] # use insted of "dat" in the code below

#Note:   if more than 50 run  receiving message code: 1 rate_limited Too many requests. Allowed '50' per minute. 429 ", the LENS API call loop was changed to accommodate this by adding waiting time between iterations (works slower but returns all available records)

#custom function for accessing LENS API
getLENSData <- function(token, query){
  url <- 'https://api.lens.org/scholarly/search'
  headers <- c('Authorization' = token, 'Content-Type' = 'application/json')
  httr::POST(url = url, add_headers(.headers=headers), body = query)
}

#prepare request
max_results <- 500 #o limit number of records per request, max currently allowed is now 500
article_list <- dat$Title #article Title
article_title <- article_list[2] #extract the first title from the list
readRenviron("~/.Renviron")
token <- Sys.getenv("LENS_TOKEN") #access token saved in the local environment

#initial query with the first title only
request <- paste0('{"query":  {"match_phrase": {"title": "', paste0('', article_title,'"'),'}}, "size": "',max_results,'","scroll": "1m"}') #, "include": ["lens_id", "authors", "publication_type", "title"] #restrict to certain fields
data <- getLENSData(token, request)
record_json <- content(data, "text")
record_list <- jsonlite::fromJSON(record_json) #convert json output from article search to list
#names(record_list)
#str(record_list$data)
print("iteration = 1")
record_df <- data.frame(record_list) #convert it into a data frame
dim(record_df)
#names(record_df)
#record_df$data.title
#record_df$data.authors
#total <- record_list[["total"]] #1

#use a loop to run query for remaining titles and add them to a data frame of records one by one
for (i in 3:length(article_list)){ 
  article_title <- article_list[i] #extract the title from the list
  request <- paste0('{"query":  {"match_phrase": {"title": "', paste0('', article_title,'"'),'}}, "size": "',max_results,'","scroll": "1m"}') #, "include": ["lens_id", "authors", "publication_type", "title"] #restrict to certain fields
  data <- getLENSData(token, request)
  print(paste("iteration = ",i))
  record_json <- content(data, "text")
  record_list <- jsonlite::fromJSON(record_json) # convert json output from article search to list
  ifelse(record_list$total==0, 
           print(paste0("record not found for: ", article_title)),
            {
            new_df <- data.frame(record_list)
            record_df <- dplyr::bind_rows(record_df, new_df) # bind the latest search data frame to the previous data frame
            }
          )
  Sys.sleep(2.5) #make it slow down to not exceed 50 calls per min
  }


dim(record_df) #15178 out of  - some titles matched multiple records
names(record_df) 
length(unique(record_df$data.title)) #11059 out of  - some titles matched multiple records, some not found
#View(cbind(article_list, record_df$data.title)) #shifted positions!

class(record_df) #data frame, but it has nested lists, etc., so cannot save as .csv

remove_titles_list <- names(sort(table(record_df$data.title), decreasing=TRUE)[1:50]) #top 50 most common titles - generic, need to remove them all
record_df2 <- record_df[!record_df$data.title %in% remove_titles_list, ]
dim(record_df2) #12159
sort(table(record_df2$data.title), decreasing=TRUE)[1:200] #still lots of generic titles (potentially remove later)
record_df <- record_df2

save(record_df, file = here("data", "LENS_dataframe.RData")) #save LENS output as a Rdata object
```

Information about LENS output values: https://docs.api.lens.org/response-scholar.html   

#######################################
## Process LENS output   - START HERE!!!!!!!!!!
```{r processing LENS output, eval = TRUE}

#start from loading save output from LENS
load(here("data", "LENS_dataframe.RData")) #loads record_df data object
#names(record_df)
#dim(record_df) #12159 34

length(unique(record_df$data.title)) #11059 unique titles
#View(record_df[duplicated(record_df$data.title) | duplicated(record_df$data.title, fromLast=TRUE), ]) #visual check - some records have more info than others
length(unique(tolower(dat_articles$Title))) #3086
length(unique(tolower(record_df$data.title))) #10846

#compare sets by title
length(intersect(unique(tolower(dat_articles$Title)), unique(tolower(record_df$data.title)))) #1938
length(setdiff(unique(tolower(dat_articles$Title)), unique(tolower(record_df$data.title)))) #1148 not found in LENS (or too different)


#fields of study
sum(sapply(record_df$data.fields_of_study, is.null)) #1385 - number of records without fields_of_study, also likely to have other missing data
record_df$data.has_fields_of_study <- sapply(record_df$data.fields_of_study, is.null)

record_df %>% arrange(data.has_fields_of_study, data.title) %>% distinct(data.title, .keep_all = TRUE) -> record_df_unique #place the records without fields_of_study at the end and remove duplicates dim(record_df_unique) #check dimensions
record_df <- record_df_unique #reassign 11009 
dim(record_df) #11009

# #Remove records manually identified as duplicates, commentaries or errors:
# remove_ids <- c("135-649-556-815-160",
# "023-784-116-518-93X", 
# "083-431-696-176-966",
# "000-329-490-788-869", 
# "161-425-757-559-60X",
# "070-142-965-217-664",
# "090-581-029-835-945",
# "096-184-264-826-781",
# "061-017-051-481-03X",
# "066-806-618-413-681",
# "150-615-763-454-941",
# "006-902-184-255-332",
# "149-960-450-431-003",
# "091-975-063-830-994",
# "144-639-707-343-162",
# "031-987-261-230-852",
# "013-843-692-321-806",
# "101-679-988-502-372",
# "130-012-333-732-368",
# "093-741-681-931-769",
# "163-791-444-765-886",
# "138-218-733-523-009",
# "188-286-702-392-550",
# "144-543-953-140-129",
# "022-505-451-384-907",
# "004-596-005-981-860",
# "193-310-481-188-721",
# "122-001-509-292-033",
# "111-262-221-567-76X",
# "132-615-996-627-724",
# "162-549-401-379-959",
# "080-102-355-958-47X",
# "131-532-176-733-767",
# "137-332-135-951-858",
# "123-619-186-219-552",
# "001-177-036-344-113",
# "184-194-140-337-897",
# "007-335-182-452-066")
# 
# record_df <- record_df[ !(record_df$data.lens_id %in% remove_ids), ]
# dim(record_df) #936

par(mar=c(4,4,2,2))
hist(record_df$data.year_published, main = "Publication year")
#min(record_df$data.year_published, na.rm = TRUE)
#max(record_df$data.year_published, na.rm = TRUE)

par(mar=c(4,15,2,2))
barplot(sort(table(record_df$data.publication_type)), horiz=TRUE, las=1, xlab = "Count", main = "Publication type")

par(mar=c(4,18,2,2))
barplot(sort(sort(table(record_df$data.source$title),decreasing = TRUE)[1:10]), horiz=TRUE, las=1, xlab = "Count", main = "Top10 publication sources (journals)")

par(mar=c(4,10,2,2))
barplot(sort(sort(table(unlist(record_df$data.fields_of_study)),decreasing = TRUE)[1:10]), horiz=TRUE, las = 1, xlab = "Count", main = "Top10 fields of study")
#record_df$data.fields_of_study[[1]][1] #first value from every list for first paper
#record_df$data.fields_of_study[1] #first value from every list for every paper

par(mar=c(4,5,2,2))
barplot(table(!is.na(record_df$data.scholarly_citations_count)), horiz=TRUE, las=1, xlab = "Count", main = "Has count of citations?") #NA should be probably 0

hist(record_df$data.scholarly_citations_count, main = "Number of citations", breaks=100) #ignoring NA
# length(unlist(record_df$data.scholarly_citations)) #45097 total citation LENS ids

par(mar=c(4,5,2,2))
barplot(table(!is.na(record_df$data.references_count)), horiz=TRUE, las=1, xlab = "Count", main = "Has references?")
# length(unlist(record_df$data.references)) #30659 total reference LENS ids

doi <- unlist(lapply(record_df$data.external_ids, function(x) x$value[match('doi', x$type)])) #extracting doi for each article

dim(record_df)
par(mar=c(4,5,2,2))
barplot(table(!is.na(doi)), horiz=TRUE, las=1, xlab = "Count", main = "Has doi?")
#table(!is.na(doi))[2] #752 have doi

#comparre sets by DOI
length(intersect(unique(dat_articles$DOI), doi)) #2287
length(setdiff(unique(dat_articles$Title), doi)) #3089 not found in LENS

```













#################################
### OLD
```{r upload original zotero references from csv, eval=TRUE}
dat <- read.csv(here("data", "ALA - Cited.csv"))
#dim(dat)
#names(dat)
```

Summarize, e.g. by document type ("Item.Type"), year ("Publication.Year"), manual tags ("Manual.Tags"), "Language", etc.   

```{r summarise overall, results = 'hide'}
names(dat)

table(dat$Item.Type, useNA = "always")
#hist(dat$Publication.Year) #make graph by year, below
table(dat$Language, useNA = "always") #needs cleaning, below
table(dat$Place, useNA = "always") #needs cleaning, see below
table(dat$Manual.Tags, useNA = "always") #split into individual later
table(is.na(dat$DOI)) #all
table(is.na(dat$References)) #none

```

```{r clean Language, results = 'hide'}

table(dat$Language, useNA = "always") #needs cleaning, below
dat$Language <- replace(dat$Language, dat$Language %in% c("en","EN", "en_AU", "en_US", "en-AU", "en-GB", "en-IN", "en-us", "en-US", "eng"), "English")
dat$Language <- replace(dat$Language, dat$Language %in% c("de"), "German")
dat$Language <- replace(dat$Language, dat$Language %in% c("fr"), "French")
dat$Language <- replace(dat$Language, dat$Language %in% c("es"), "Spanish")
dat$Language <- replace(dat$Language, dat$Language %in% c("por"), "Portugese")
dat$Language <- replace(dat$Language, dat$Language %in% c("IT"), "Italian")
table(dat$Language, useNA = "always") #much better
table(dat$Language)["English"]
table(dat$Language)[""]
sum(dat$Language=="")

```

Language: `r table(dat$Language)["English"]` out of `r sum(table(dat$Language))` are in English, but for `r  sum(dat$Language=="")` records the language info is missing.  

```{r clean Place, eval=FALSE}
table(dat$Place, useNA = "always") #needs cleaning, below
all_countries <- str_c(unique(iso3166$sovereignty), collapse = "|") #use iso3166 data set
df <- dat
#all_countries <- str_c(unique(world.cities$country.etc), collapse = "|")
df$country <- sapply(str_extract_all(df$Place, all_countries), toString)
table(df$country, useNA = "always") #needs much more cleaning - try again after checking doi overlap with GBIF data set, which is much cleaner
```

**TODO:**    
- country information needs extensive cleaning - try again after checking doi overlap with GBIF data set, which is much cleaner and contains both author and actual research location data.  
- then fill in gaps in the ALA data.   


### Plot year and literature type information      

```{r plot year , fig.width = 10, fig.height = 10, fig.cap = "Figure 1.", results = 'hide'}

dat %>% 
  count(Publication.Year, Item.Type) -> count_year

nb.cols <- 14
mycolors <- colorRampPalette(brewer.pal(8, "Set2"))(nb.cols)

ggplot(count_year, aes(as.character(Publication.Year), n, fill=Item.Type)) + 
  geom_bar(stat="identity") + #, position="dodge"
  xlab("") + 
  scale_fill_manual(values = mycolors)

```

Plot by literature type.
  
```{r plot data type, fig.width = 10, fig.height = 10, fig.cap = "Figure 2.", results = 'hide', include = FALSE}
### Summarize literature type information  
#par(mar = c(2, 10, 2, 2))
#barplot(table(dat$Item.Type), horiz = TRUE, las=1,  main = "Number of papers per species") #how many articles per species

dat %>% 
  count(Item.Type) -> count_lit_type

ggplot(count_lit_type, aes(x = reorder(Item.Type, n), y = n)) + geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") 

```


Subset by year - for piloting the code only use data from 2020.     

```{r subset articles from 2020, results = 'hide'}
dat_2020 <- dat %>% filter(Publication.Year == 2020) 
dim(dat_2020)
```

Subset journal articles.   

```{r subset articles, results = 'hide'}
dat_articles <- dat_2020 %>% filter(Item.Type == "journalArticle") %>% select( c(Key:Manual.Tags))
dim(dat_articles)

#remove remaining completely empty columns
dat_articles %>% select_if(function(x){!all(is.na(x))}) -> dat_articles

#table(dat_articles$Manual.Tags, useNA = "always")
```

Make a data missingnes plot.   

```{r missingness plot, fig.width = 10, fig.height = 6, fig.cap = "Figure. Distribution of missing values in journal articles data subset", results = 'hide'}
vis_miss(dat_articles) +
  theme(plot.title = element_text(hjust = 0.5, vjust = 3), 
        plot.margin = margin(t = 0.5, r = 3, b = 1, l = 1, unit = "cm")) +
  ggtitle("Missing data for dat_articles") #no missing values
```

Data is complete, however this needs further checking (e.g. special characters used for missing values).   


Use DOI to integrate with additional data available from Unpaywall via [roadoi R package](https://docs.ropensci.org/roadoi/reference/roadoi-package.html).   

```{r article doi data, results = 'hide', eval = FALSE}
#table(is.na(dat_articles$DOI)) #no missing DOI values
dois <- unique(dat_articles$DOI) #some values need cleaning, as not proper DOI (empty strings)


refs_data <- roadoi::oadoi_fetch(dois = dois, email = "losialagisz@gmail.com") #get refs data from doi
#names(refs_data)

#get failed DOIs
#str(warnings()) #List of 28
#failed_DOIs <- warnings()

#refs_data$doi
#dat_articles$DOI

dat_articles$doi <- dat_articles$DOI
dat_articles$doi <- gsub("-", NA, dat_articles$doi, fixed=TRUE)

#match and merge data to dat_articles by DOI
dat_articles_refs <- left_join(dat_articles, refs_data, by="doi")
dim(dat_articles_refs)
#str(dat_articles_refs)
#names(dat_articles_refs)

#table(dat_articles_refs$is_oa, useNA = "always")
readr::write_csv(dat_articles_refs, file = here("data","dat_articles_refs.csv"))

```

refs_data - Authors

```{r process author names and affiliations, results = 'hide', eval = FALSE}
str(refs_data$authors[[2]])
str(refs_data$affiliations[[1]])
refs_data$authors[[21]]

sum(is.na(dat_articles_refs$authors))
dat_articles_refs$authors[15]
```


**TODO:**    
- more cleaning up data, esp manual tags and location   
- get full bibliometric records for articles, including references and affiliations    
- get journal IF and ranking values  
- get lists of citing and cited articles - e.g. from LENS to use for citation networks (articles only)    
- convert literature data into the internal *bibliometrix* format  
- run network and topic analyses  
 

**TODO:**      
- check how much of this literature is captured by ALA literature data set   
- process in a way that the data can be used to visualize overlaps and differences   

----
